
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from __future__ import division
import glob
import matplotlib.pyplot as plt
import skimage.io
import skimage.color
import skimage.filters
import cv2
from scipy import ndimage
from skimage import io, color, measure, exposure, data
from skimage.measure import label, regionprops, regionprops_table
import pandas as pd
import matplotlib.pyplot as plt
import os
import scipy.ndimage 
import IPython.display as ipd 
from tqdm.notebook import tqdm
import subprocess
from scipy.ndimage import gaussian_filter
from skimage.filters import rank
from skimage import data
from skimage.filters import *
import pickle
from PIL import Image

#concatenate excel files in case different measurement had been done and they need to merge
folder_path = r'Path_of_the_folder_with_all_csv_files'

dfs = []

for filename in os.listdir(folder_path):
    if filename.endswith(".csv"):  # You can change the file extension as needed
        file_path = os.path.join(folder_path, filename)
        df = pd.read_csv(file_path)
        dfs.append(df)

final_df = pd.concat(dfs, ignore_index=True)
final_df

#Save the concatenated data in csv format
final_df.to_csv('file_name.csv', index=False)


#Read file/s
df_file= pd.read_csv(r'file_path')
df_file

#extra step to modify data. Skip it if don't need it. 
# Define the columns and their respective thresholds
columns_and_thresholds = {'major_axis_length': 0.8, 'area': 2.0}

# Loop through the columns and replace values based on their thresholds
for col, threshold in columns_and_thresholds.items():
    filter_sonication[col] = filter_sonication[col].apply(lambda x: threshold if x < threshold else x)

# Save the modified DataFrame as a CSV file
filter_sonication.to_csv('file_name.csv', index=False)


# get the purity of the sorted subpopulations by dividing the data into four regions of larger and smaller than critical size and larger and smaller than 0.5 in circularity. The same region that are shown in scatter plots

#filtering can be based on different properties. Here is the example of major_axis_length and circularity which are from Pandas dataframe
# Filter based on major_axis_length
filtered_major = df_file[
    #(df_file['major_axis_length'] > 0.8) &
    (df_file['major_axis_length'] < 8.5)
]

# Filter based on eccentricity
filtered_eccentricity = df_file[df_file['circularity'] < 0.5]
# Filter based on area
#filtered_area = df_file[df_file['area'] > 2]

# Calculate the length of the selected DataFrame
selected_length = len(filtered_major.index & filtered_eccentricity.index) #& filtered_area.index)

# Calculate the total length of the original DataFrame
total_length = len(df_file)

# Calculate the percentage of selected rows
percentage_selected = (selected_length / total_length) * 100

print(f"Percentage of selected rows: {percentage_selected:.2f}%")
        
#Getting the basic statistics out of data. Different properties can be analyzed
mean = df_file['major_axis_length'].mean()
median = df_file['major_axis_length'].median()
mode = df_file['major_axis_length'].mode().values[0]
std_dev = df_file['major_axis_length'].std()

#statistical analysis batch processing
def stat(all_df): #all_df--> read all the dataframes that you want to compare and put them in a numppy array like this: all_dff=[df_file, df_file_2, df_file_3 ]
    #min_values = []
    max_values = []
    median = []
    mean = []
    for df in all_df:
        for col in df.columns[0:4]: #change the number of columns according to the dataset
             #min_val = df[col].min()
             max_val = df[col].max()
             mean_val=df[col].mean()
             median_val=df[col].median()
             
            # min_values.append(min_val)
             max_values.append(max_val)
             mean.append(mean_val)
             median.append(median_val)
    return print ( 'The max is', max_values, 'The mean is', mean, 'The median is', median)
        
#Testing the function
stat(all_df=all_dff) 



# Plotting the histogram. #different properties can be chosen to plot. Here is major_axis_length. Number of subplots depends of the number of data that are needed to compare.
fig,axs=plt.subplots(1,1,figsize=(30,18))

plt.hist(df_file['major_axis_length'], bins=40, edgecolor='black', color='yellow', density='True')
#plt.hist(df_file_2['major_axis_length'], bins=10, edgecolor='black', color='red')
#plt.hist(df_file_3['major_axis_length'], bins=10, edgecolor='black', color='green')
#plt.hist(df_file_4['major_axis_length'], bins=40, edgecolor='black', color='Orange')
#plt.title('Histogram of Major_axis_length')
plt.xlabel('Major axis length', fontsize=30)
plt.ylabel('Frequency', fontsize=30)
plt.xlim(0, 70) #depends on the data size
plt.tick_params(axis='both', labelsize=24)
plt.savefig('file_name.pdf')


#plotting histogram with the same bin size

# Define bin edges based on the desired size of bins
bin_edges = np.linspace(0, 75, 41)  # 40 bins with equal width from 0 to 75. These values can be adjusted depending on data range

fig, axs = plt.subplots(5, 1, figsize=(30, 18))

axs[0].hist(df_file['major_axis_length'], bins=bin_edges, edgecolor='black', color='green', density=True, label='Inlet')
axs[1].hist(df_file_2['major_axis_length'], bins=bin_edges, edgecolor='black', color='red', density=True, label='Displaced')
axs[2].hist(df_file_3['major_axis_length'], bins=bin_edges, edgecolor='black', color='orange', density=True, label='Moderately displaced')
axs[3].hist(df_file_4['major_axis_length'], bins=bin_edges, edgecolor='black', color='blue', density=True, label='Non-displaced')
axs[4].hist(df_file_5['major_axis_length'], bins=bin_edges, edgecolor='black', color='yellow', density=True, label='Sum of outlets')

for ax in axs:
    ax.set_xlabel('Major axis length', fontsize=30)
    ax.set_ylabel('Frequency', fontsize=30)
    ax.set_xlim(0, 75)
    ax.legend(fontsize=30)
    ax.tick_params(axis='both', labelsize=24)

plt.tight_layout()
#Save file in pdf or png format
plt.savefig('file_name.pdf')
plt.show()

#Measuring total pixel value of images to calculate comcentration
def measure_pixels(folder_path):
   total_area = 0
   for filename in os.listdir(folder_path):
        if filename.endswith(".png") or filename.endswith(".tif") or filename.endswith(".jpeg"):
            image_path = os.path.join(folder_path, filename)
            img = Image.open(image_path)
            width, height = img.size
            total_area += width * height

   return total_area

# Example usage:
folder_path = r"folder_path"  
total_area = measure_pixels(folder_path)
print(f"Sum of products of width and height of all images: {total_area*0.65*0.65/1000000}") #The final value would be in um2


#Measuring concentration
concentration_of_objects=len(df_file)/(total_area_in_um)  
concentration_of_bacteria = df_file['area'].sum()/(area_of_one_bacteria)  


# Plotting concentration_of_objects and concentration_of_bacteria nside by side in bar charts
fig, ax = plt.subplots(figsize=(14, 10))

# Set the width of the bars
bar_width = 0.2

# Set the positions of the bars on the x-axis
r1 = np.arange(len(df.columns))
r2 = [x + bar_width for x in r1]

# Plot the bars
ax.bar(r1, df_concentration_of_objects.iloc[0], color='b', width=bar_width, edgecolor='grey', label='Number of particles/um2')
ax.bar(r2, df_concentration_of_bacteria.iloc[1], color='r', width=bar_width, edgecolor='grey', label='Number of bacteria/um2')

# Add labels and title
#ax.set_xlabel('Columns', fontsize=30)
ax.set_ylabel('Number of bacteria/particles', fontsize=30)
ax.set_xticks([r + bar_width/2 for r in r1])
ax.set_xticklabels(df.columns)
ax.legend(fontsize=24)
ax.tick_params(axis='both', labelsize=24)
ax.set_yscale('log')

plt.xticks(rotation=0)
plt.tight_layout()

# Show plot
plt.savefig('file_name.pdf')


#Boxplot
# Filter the dataframes. Skip it if you don't need it
filtered_df = df_file[(inlet['major_axis_length'] > 0.8) & (inlet['major_axis_length'] < 75)]
#And the rest of the dataframes...



# Plotting
fig, ax = plt.subplots(figsize=(14, 10))

# Create boxplot for each dataframe. Number of subplots can vary according to number of dataframe
df_file.boxplot(ax=ax, column=['major_axis_length'], positions=[1], widths=0.2, patch_artist=True, labels=['label_1'])
df_file_2.boxplot(ax=ax, column=['major_axis_length'], positions=[2], widths=0.2, patch_artist=True, labels=['label_2'])
df_file_3.boxplot(ax=ax, column=['major_axis_length'], positions=[3], widths=0.2, patch_artist=True, labels=['label_3'])
df_file_4.boxplot(ax=ax, column=['major_axis_length'], positions=[4], widths=0.2, patch_artist=True, labels=['label_4'])
df_file_5.boxplot(ax=ax, column=['major_axis_length'], positions=[5], widths=0.2, patch_artist=True, labels=['label_5'])
# Remove grid
ax.grid(False)
# Set labels and title
#ax.set_xlabel('x_label', fontsize=30)
#ax.set_ylabel('y_label', fontsize=30)
ax.tick_params(axis='both', labelsize=24)
#ax.legend(box1["boxes"][0], box2["boxes"][0], box3["boxes"][0], box4["boxes"][0], box5["boxes"][0], ['label_1', 'label_2', 'label_3','label_4','label_5'], fontsize=24
# Show plot
plt.savefig('file_name.pdf')



#Scatter plots function which also includes vurve fitting. comment curve fitting if don't need it. The function has three inputs. One can increase or decrease it according to the sets of data frames.
import matplotlib.colors as mcolors
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

def ploting(x_d, y_d, x_m, y_m, x_n, y_n, c_d, c_m, c_n): 
    
    x_values = [x_d , x_m, x_n] #
    y_values = [y_d, y_m,  y_n] #
    c_values = [c_d , c_m, c_n] #

    fig, axs = plt.subplots(3, 1, figsize=(14, 12))  # Smaller figsize
    
    for i, ax in enumerate(axs.flat):
        x = x_values[i]
        y = y_values[i]
        c = c_values[i]
        x = np.maximum(x, 1e-10)
        
        scat = ax.scatter(x, y, c=c, cmap='rainbow')
       
        # Set colorbar scale to log scale
        norm = mcolors.LogNorm(vmin=np.min(c), vmax=np.max(c))
        cbar = fig.colorbar(scat, ax=ax) #add norm=norm if want to normalize the rainbow graph
        cbar.ax.set_ylabel('Circularity', fontsize=14)
        scat.set_clim(1, 300) #chnage it accordingly
        
        # Fit a non-linear curve to the data
        #popt, _ = curve_fit(logarithmic_function, x, y)
        #ax.plot(x, logarithmic_function(x, *popt), color='black', label='Non-linear fit', linewidth=0.1)
        
        ax.set_xlabel('Major axis', fontsize=14)
        ax.set_ylabel('Circularity', fontsize=14)
        ax.set_xlim(8.5,20)
        ax.set_ylim(0, 1.2)
       # ax.set_yscale('log')
        ax.tick_params(axis='both', which='major', labelsize=12)
        ax.tick_params(axis='both', which='minor', labelsize=12)
       # ax.set_yticks(np.arange(0, 1.2, 0.1))  # Adjust max_y_value and step_size as needed
        ax.set_title(['df_file', 'df_file_1', 'df_file_2, fontsize=14) #

    fig.tight_layout(pad=3.0)

    return plt.savefig('file_name.pdf')

#Non-linear function to call in th eplotting function for curve fitting
def nonlinear_function(x, a, b, c):
    # Define the non-linear function (e.g., quadratic)
    return a * x**2 + b * x + c

# Define the logarithmic function you want to fit
def logarithmic_function(x, a, b):
    return a * np.log(x) + b

def exponential_function(x, a, b):
    return a * np.exp(b * x)


def power_function(x, a, b):
    return a * x**b

def polynomial_function(x, *coefficients):
    return np.polyval(coefficients, x)



