# -*- coding: utf-8 -*-
"""
Created on Wed Feb  7 16:11:52 2024

@author: el4003ak
"""


# import the necessary packages
from __future__ import print_function
from skimage.feature import peak_local_max
from skimage.segmentation import watershed
from scipy import ndimage
import argparse
import imutils
import cv2
import argparse
from __future__ import division
import numpy as np
import glob
import matplotlib.pyplot as plt
import skimage.io
import skimage.color
import skimage.filters
from scipy import ndimage
from skimage import io, color, measure, exposure, data
from skimage.measure import label, regionprops, regionprops_table
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import scipy.ndimage 
import IPython.display as ipd 
from tqdm.notebook import tqdm
import subprocess
import skimage.io 
from skimage import filters
from scipy.ndimage import gaussian_filter
from skimage.filters import rank
from skimage import data
from skimage.filters import *
import pickle

image_path = r'copy the path here'


def watershed_function(image_path):
    # Load the image
    image = cv2.imread(image_path)
    
    # Apply mean shift filtering
    shifted = cv2.pyrMeanShiftFiltering(image, 21, 51)
    
    # Convert the shifted image to grayscale
    gray = cv2.cvtColor(shifted, cv2.COLOR_BGR2GRAY)
    
    # Perform thresholding (Otsu's method), Other thresholding maybe suitable depending on the segmentation quality
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    
    
    D = ndimage.distance_transform_edt(thresh)
    localMax = peak_local_max(D, indices=False, min_distance=20,
    	labels=thresh)
    # perform a connected component analysis on the local peaks,
    # using 8-connectivity, then appy the Watershed algorithm
    markers = ndimage.label(localMax, structure=np.ones((3, 3)))[0]
    labels = watershed(-D, markers, mask=thresh)

    #color labelling
    s = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]
    imgcolor = color.label2rgb(labels, bg_label=0)
    
    # Measure region properties
    props = measure.regionprops(labels)
    props_table = measure.regionprops_table(labels, properties=('area', 'eccentricity', 'major_axis_length', 'minor_axis_length'))
    
    # Display the original image, thresholded image and labelled image 
    fig, ax = plt.subplots(1, 3, figsize=(12, 6))
    ax[0].imshow(image, cmap='gray')
    ax[0].set_title('Imagee')
    ax[1].imshow(thresh, cmap='gray')
    ax[1].set_title('Threshold')
    ax[2].imshow(imgcolor, cmap='gray')
    ax[2].set_title('Labelled image')
    plt.savefig('output_all.pdf')
    
    return props_table


#Batch processing

def batch_processing_watershed(input_dir):  #input_dir=the folder having targeted files
    
    filelist = glob.glob(input_dir + '/*.tif')
    

    all_props = []
    success_counter = 0
    
    for filename in filelist:
        # Exception handling so the program can move on if one image fails for some reason.
        try:
           props_table= watershed_final(filename)   #imgcolor, props_table, histogram, labeled_mask,
           all_props.append(props_table)
           success_counter += 1
            
            # Update the success counter       
        except Exception:
            from warnings import warn
            warn("There was an exception in " + filename + "!!!")
     
    # How many images were successfully analyzed?
    print ("Successfully analyzed", success_counter, "of", len(filelist), "images")
    
    return  all_props
# The output of the batch processor is props table concatenating the table from all images.

#Post processing image analyzing data
all_props=batch_processing_watershed(input_dir=r'folder_path_here')
    
all_properties = pd.concat([pd.DataFrame(props) for props in all_props], ignore_index=True)

all_properties = all_properties.replace(0, pd.NA).dropna()

# Calculating circularity
all_properties['circularity'] = 4 * all_properties['area'] / \
                             (all_properties['major_axis_length'] + 
                              all_properties['minor_axis_length'])**2

all_properties['circularity']=all_properties['circularity']*(4/np.pi)

#Change the scale to um by putting the pixel size values
all_properties['area']=all_properties['area']*pixel_size_in_micrometer*pixel_size_in_micrometer
all_properties['major_axis_length']=all_properties['major_axis_length']*pixel_size_in_micrometer
all_properties['minor_axis_length']=all_properties['minor_axis_length']*pixel_size_in_micrometer
all_properties

#Data filtering if needed.
all_properties = all_properties.loc[all_properties['circularity'] >= 0.8]
all_properties = all_properties.loc[all_properties['major_axis_length']>=3]
all_properties = all_properties.loc[all_properties['major_axis_length']<=100]
all_properties = all_properties.loc[all_properties['minor_axis_length']>=1]

#Saving final pandas dataframe into excel (.csv) file
all_properties.to_csv('file_name.csv', index=False)

